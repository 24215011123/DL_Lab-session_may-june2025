{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNJc5HDEzleY5yzA21tPi3b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/24215011123/DL_Lab-session_may-june2025/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: Imports & config"
      ],
      "metadata": {
        "id": "pdGRYI5Vjg_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Full pipeline\n",
        "# 70/15/15 split, preprocess fitted on train only, SMOTE on train only\n",
        "# Models: LogReg, RF, XGB, LightGBM, ANN, DeepMLP, TabTransformer\n",
        "# Saves outputs to ./outputs\n",
        "# =========================\n",
        "\n",
        "# Section 0: Imports & config\n",
        "import os, json\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, average_precision_score, brier_score_loss,\n",
        "                             confusion_matrix, roc_curve, precision_recall_curve)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import shap\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Seeds & directories\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "OUTDIR = \"outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# Runtime knobs (lower for debug)\n",
        "EPOCHS_TAB = 50\n",
        "EPOCHS_DEEPMLP = 50\n",
        "BATCH_TAB = 512\n",
        "BATCH_DEEPMLP = 512"
      ],
      "metadata": {
        "id": "DDmIFuGng7DS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: Load data"
      ],
      "metadata": {
        "id": "qStWcqZUjcPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Load data\n",
        "DATA_PATH = \"/content/heart_disease_health_indicators_BRFSS2015.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Raw shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX-Nv440g7Fm",
        "outputId": "6a36a508-e31d-43a0-c9e6-9f87d540c765"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shape: (253680, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NvhJKYGsg7IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2–5: Features + Preprocessing + Split + SMOTE"
      ],
      "metadata": {
        "id": "19c1syxBkERg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Section 2–5: Features + Preprocessing + Split + SMOTE\n",
        "# =========================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# --- Define feature groups ---\n",
        "binary_cols = [\"HighBP\",\"HighChol\",\"CholCheck\",\"Smoker\",\"Stroke\",\"Diabetes\",\n",
        "               \"PhysActivity\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\n",
        "               \"AnyHealthcare\",\"NoDocbcCost\",\"DiffWalk\",\"Sex\"]\n",
        "\n",
        "numeric_cols = [\"BMI\",\"MentHlth\",\"PhysHlth\",\"Age\"]\n",
        "\n",
        "categorical_cols = [\"GenHlth\",\"Education\",\"Income\"]\n",
        "\n",
        "all_cols = binary_cols + numeric_cols + categorical_cols\n",
        "print(\"Using features (count):\", len(all_cols))\n",
        "\n",
        "# --- Target & raw features ---\n",
        "TARGET = \"HeartDiseaseorAttack\"\n",
        "y_raw = df[TARGET].values.astype(int)   # ensure integers\n",
        "X_raw = df[all_cols].copy()\n",
        "\n",
        "print(\"Raw shape:\", X_raw.shape)\n",
        "\n",
        "# --- Split 70/15/15 ---\n",
        "X_train_raw, X_temp_raw, y_train_raw, y_temp_raw = train_test_split(\n",
        "    X_raw, y_raw, test_size=0.30, stratify=y_raw, random_state=SEED\n",
        ")\n",
        "X_val_raw, X_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
        "    X_temp_raw, y_temp_raw, test_size=0.50, stratify=y_temp_raw, random_state=SEED\n",
        ")\n",
        "print(\"Raw splits:\", X_train_raw.shape, X_val_raw.shape, X_test_raw.shape)\n",
        "\n",
        "# --- Preprocessing with Imputation ---\n",
        "num_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "cat_transformer = Pipeline(steps=[\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", num_transformer, numeric_cols),\n",
        "    (\"bin_cat\", cat_transformer, binary_cols + categorical_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "# Fit only on train\n",
        "preprocessor.fit(X_train_raw)\n",
        "\n",
        "X_train = preprocessor.transform(X_train_raw)\n",
        "X_val   = preprocessor.transform(X_val_raw)\n",
        "X_test  = preprocessor.transform(X_test_raw)\n",
        "\n",
        "# Ensure dense\n",
        "if hasattr(X_train, \"toarray\"):\n",
        "    X_train = X_train.toarray()\n",
        "    X_val   = X_val.toarray()\n",
        "    X_test  = X_test.toarray()\n",
        "\n",
        "print(\"Processed shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"NaN check:\", np.isnan(X_train).sum(), np.isnan(X_val).sum(), np.isnan(X_test).sum())\n",
        "\n",
        "# --- SMOTE on training only ---\n",
        "sm = SMOTE(random_state=SEED)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train_raw)\n",
        "print(\"After SMOTE:\", X_train_res.shape, np.bincount(y_train_res.astype(int)))\n",
        "\n",
        "# --- Save feature names ---\n",
        "FEAT_NAMES = numeric_cols + binary_cols + categorical_cols\n",
        "pd.Series(FEAT_NAMES).to_csv(os.path.join(OUTDIR,\"features_used.csv\"), index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcZNSP20g7K6",
        "outputId": "2098e1bd-c3b5-4417-bb48-f357b73a372b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using features (count): 21\n",
            "Raw shape: (253680, 21)\n",
            "Raw splits: (177576, 21) (38052, 21) (38052, 21)\n",
            "Processed shapes: (177576, 21) (38052, 21) (38052, 21)\n",
            "NaN check: 0 0 0\n",
            "After SMOTE: (321702, 21) [160851 160851]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtNXoLxog7Nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6: Metric helpers"
      ],
      "metadata": {
        "id": "fwiXxDIGjSBo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Section 6: Metric helpers\n",
        "def compute_metrics(y_true, probs, thresh=0.5):\n",
        "    preds = (probs >= thresh).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, preds)),\n",
        "        \"prec\": float(precision_score(y_true, preds, zero_division=0)),\n",
        "        \"rec\": float(recall_score(y_true, preds, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, preds, zero_division=0)),\n",
        "        \"auc\": float(roc_auc_score(y_true, probs)),\n",
        "        \"ap\": float(average_precision_score(y_true, probs)),\n",
        "        \"brier\": float(brier_score_loss(y_true, probs))\n",
        "    }"
      ],
      "metadata": {
        "id": "2XGRn8w_g7RI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 7: Train classical models on X_train_res / evaluate on X_test"
      ],
      "metadata": {
        "id": "ljVWgfwqjM2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Section 7: Train classical models on X_train_res / evaluate on X_test\n",
        "results = {}\n",
        "\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=SEED)\n",
        "logreg.fit(X_train_res, y_train_res)\n",
        "probs_log = logreg.predict_proba(X_test)[:,1]\n",
        "metrics_log = compute_metrics(y_test_raw, probs_log)\n",
        "cm_log = confusion_matrix(y_test_raw, (probs_log>=0.5).astype(int))\n",
        "results['LogReg'] = metrics_log\n",
        "print(\"LogReg metrics:\", metrics_log)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=300, class_weight=\"balanced\", n_jobs=-1, random_state=SEED)\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "probs_rf = rf.predict_proba(X_test)[:,1]\n",
        "metrics_rf = compute_metrics(y_test_raw, probs_rf)\n",
        "cm_rf = confusion_matrix(y_test_raw, (probs_rf>=0.5).astype(int))\n",
        "results['RandomForest'] = metrics_rf\n",
        "print(\"RF metrics:\", metrics_rf)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=SEED, n_jobs=-1)\n",
        "xgb_model.fit(X_train_res, y_train_res)\n",
        "probs_xgb = xgb_model.predict_proba(X_test)[:,1]\n",
        "metrics_xgb = compute_metrics(y_test_raw, probs_xgb)\n",
        "cm_xgb = confusion_matrix(y_test_raw, (probs_xgb>=0.5).astype(int))\n",
        "results['XGBoost'] = metrics_xgb\n",
        "print(\"XGB metrics:\", metrics_xgb)\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(random_state=SEED)\n",
        "lgb_model.fit(X_train_res, y_train_res)\n",
        "probs_lgb = lgb_model.predict_proba(X_test)[:,1]\n",
        "metrics_lgb = compute_metrics(y_test_raw, probs_lgb)\n",
        "cm_lgb = confusion_matrix(y_test_raw, (probs_lgb>=0.5).astype(int))\n",
        "results['LightGBM'] = metrics_lgb\n",
        "print(\"LGB metrics:\", metrics_lgb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52Iayy-EgyJU",
        "outputId": "96ad12ee-e1d9-4dbd-ed31-c8a4ede534a0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LogReg metrics: {'acc': 0.7548617681068012, 'prec': 0.24731655815590356, 'rec': 0.7843191964285714, 'f1': 0.37605351170568563, 'auc': 0.8446944497043841, 'ap': 0.36197065181252813, 'brier': 0.1626198876815193}\n",
            "RF metrics: {'acc': 0.8966151582045622, 'prec': 0.40179573512906847, 'rec': 0.19977678571428573, 'f1': 0.2668654491241148, 'auc': 0.8231896105885999, 'ap': 0.30583578408017187, 'brier': 0.0774669397596344}\n",
            "XGB metrics: {'acc': 0.9059970566593083, 'prec': 0.5035246727089627, 'rec': 0.13950892857142858, 'f1': 0.2184837229626393, 'auc': 0.8436233216187065, 'ap': 0.3582914260042247, 'brier': 0.07134264188398001}\n",
            "[LightGBM] [Info] Number of positive: 160851, number of negative: 160851\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.051730 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5355\n",
            "[LightGBM] [Info] Number of data points in the train set: 321702, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "LGB metrics: {'acc': 0.9048933038999264, 'prec': 0.48610007942811756, 'rec': 0.17075892857142858, 'f1': 0.25273590749535413, 'auc': 0.8451178618120431, 'ap': 0.36012670452080836, 'brier': 0.07171741520495813}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aLXW0kh7TVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 8: Neural baselines (Keras)"
      ],
      "metadata": {
        "id": "hmpYMDyrj1AA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 8: Neural baselines (Keras)\n",
        "input_dim = X_train_res.shape[1]\n",
        "\n",
        "# Simple ANN\n",
        "ann = Sequential([Dense(32, activation='relu', input_shape=(input_dim,)), Dense(1, activation='sigmoid')])\n",
        "ann.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "ann.fit(X_train_res, y_train_res, validation_data=(X_val, y_val_raw), epochs=50, batch_size=512, verbose=1)\n",
        "probs_ann = ann.predict(X_test).flatten()\n",
        "metrics_ann = compute_metrics(y_test_raw, probs_ann)\n",
        "cm_ann = confusion_matrix(y_test_raw, (probs_ann>=0.5).astype(int))\n",
        "results['ANN'] = metrics_ann\n",
        "print(\"ANN metrics:\", metrics_ann)\n",
        "\n",
        "# Deep MLP\n",
        "deep_mlp = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "    BatchNormalization(), Dropout(0.3),\n",
        "    Dense(128, activation='relu'), BatchNormalization(), Dropout(0.3),\n",
        "    Dense(64, activation='relu'), BatchNormalization(), Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "deep_mlp.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "deep_mlp.fit(X_train_res, y_train_res, validation_data=(X_val, y_val_raw), epochs=EPOCHS_DEEPMLP, batch_size=BATCH_DEEPMLP, verbose=1)\n",
        "probs_dmlp = deep_mlp.predict(X_test).flatten()\n",
        "metrics_dmlp = compute_metrics(y_test_raw, probs_dmlp)\n",
        "cm_dmlp = confusion_matrix(y_test_raw, (probs_dmlp>=0.5).astype(int))\n",
        "results['DeepMLP'] = metrics_dmlp\n",
        "print(\"DeepMLP metrics:\", metrics_dmlp)\n",
        "\n",
        "# Save intermediate results\n",
        "pd.DataFrame(results).T.to_csv(os.path.join(OUTDIR, \"results_before_tab.csv\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fItYtO29gyGi",
        "outputId": "42ec109f-0fca-462a-adf7-b0f5efdb96cc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.5298 - val_loss: 0.4812\n",
            "Epoch 2/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.4644 - val_loss: 0.4750\n",
            "Epoch 3/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 0.4598 - val_loss: 0.4709\n",
            "Epoch 4/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4560 - val_loss: 0.4670\n",
            "Epoch 5/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.4527 - val_loss: 0.4650\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "ANN metrics: {'acc': 0.7617733627667402, 'prec': 0.2513383540513565, 'rec': 0.7728794642857143, 'f1': 0.3793221499486477, 'auc': 0.8439556125557452, 'ap': 0.3586664068354871, 'brier': 0.15612085764817457}\n",
            "Epoch 1/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.5033 - val_loss: 0.5491\n",
            "Epoch 2/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.4601 - val_loss: 0.4804\n",
            "Epoch 3/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.4508 - val_loss: 0.4231\n",
            "Epoch 4/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.4421 - val_loss: 0.4191\n",
            "Epoch 5/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.4326 - val_loss: 0.3977\n",
            "Epoch 6/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.4226 - val_loss: 0.4159\n",
            "Epoch 7/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.4123 - val_loss: 0.3889\n",
            "Epoch 8/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.4044 - val_loss: 0.4115\n",
            "Epoch 9/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3964 - val_loss: 0.3958\n",
            "Epoch 10/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3905 - val_loss: 0.3801\n",
            "Epoch 11/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.3846 - val_loss: 0.3832\n",
            "Epoch 12/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 0.3796 - val_loss: 0.3711\n",
            "Epoch 13/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3759 - val_loss: 0.3721\n",
            "Epoch 14/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3719 - val_loss: 0.3668\n",
            "Epoch 15/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3664 - val_loss: 0.3446\n",
            "Epoch 16/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3636 - val_loss: 0.3339\n",
            "Epoch 17/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3583 - val_loss: 0.3727\n",
            "Epoch 18/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.3551 - val_loss: 0.3448\n",
            "Epoch 19/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3519 - val_loss: 0.3323\n",
            "Epoch 20/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3507 - val_loss: 0.3241\n",
            "Epoch 21/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3476 - val_loss: 0.3349\n",
            "Epoch 22/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3429 - val_loss: 0.3376\n",
            "Epoch 23/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3422 - val_loss: 0.3515\n",
            "Epoch 24/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3395 - val_loss: 0.3173\n",
            "Epoch 25/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3382 - val_loss: 0.3084\n",
            "Epoch 26/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3371 - val_loss: 0.3332\n",
            "Epoch 27/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 0.3354 - val_loss: 0.3420\n",
            "Epoch 28/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3314 - val_loss: 0.3623\n",
            "Epoch 29/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3292 - val_loss: 0.3298\n",
            "Epoch 30/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3271 - val_loss: 0.3318\n",
            "Epoch 31/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3283 - val_loss: 0.3013\n",
            "Epoch 32/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3260 - val_loss: 0.3363\n",
            "Epoch 33/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3241 - val_loss: 0.3303\n",
            "Epoch 34/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3243 - val_loss: 0.3089\n",
            "Epoch 35/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3227 - val_loss: 0.3264\n",
            "Epoch 36/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3223 - val_loss: 0.3070\n",
            "Epoch 37/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3207 - val_loss: 0.3428\n",
            "Epoch 38/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3191 - val_loss: 0.3217\n",
            "Epoch 39/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3180 - val_loss: 0.3357\n",
            "Epoch 40/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3179 - val_loss: 0.3022\n",
            "Epoch 41/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3151 - val_loss: 0.3047\n",
            "Epoch 42/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3140 - val_loss: 0.3101\n",
            "Epoch 43/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3136 - val_loss: 0.3120\n",
            "Epoch 44/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.3140 - val_loss: 0.3101\n",
            "Epoch 45/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3141 - val_loss: 0.3149\n",
            "Epoch 46/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3133 - val_loss: 0.3220\n",
            "Epoch 47/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.3109 - val_loss: 0.3218\n",
            "Epoch 48/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 0.3096 - val_loss: 0.3155\n",
            "Epoch 49/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.3100 - val_loss: 0.3069\n",
            "Epoch 50/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.3082 - val_loss: 0.3066\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step\n",
            "DeepMLP metrics: {'acc': 0.8614790286975718, 'prec': 0.33353068877047565, 'rec': 0.47154017857142855, 'f1': 0.39070627673101377, 'auc': 0.828692555413717, 'ap': 0.33146385274975726, 'brier': 0.09616857707398696}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 9: TabTransformer (single split; robust attention)"
      ],
      "metadata": {
        "id": "AR8L__92jH8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Section 9: TabTransformer (single split; robust attention)\n",
        "class TabDatasetTorch(Dataset):\n",
        "    def __init__(self, X_np, y_np):\n",
        "        self.X = torch.tensor(X_np, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_np, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "class TabTransformerTorch(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim=16, n_heads=2, mlp_hidden=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(1, emb_dim)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.ReLU())\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim*emb_dim, mlp_hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b,f = x.shape\n",
        "        x = x.unsqueeze(-1)            # (b,f,1)\n",
        "        emb = self.proj(x)             # (b,f,emb_dim)\n",
        "        attn_out, attn_weights = self.mha(emb, emb, emb, need_weights=True, average_attn_weights=False)\n",
        "        out = self.ff(emb + attn_out)\n",
        "        flat = out.reshape(b, -1)\n",
        "        logits = self.classifier(flat)\n",
        "        return logits, attn_weights, out\n",
        "\n",
        "def evaluate_tabtransformer_single_split(X_train_res, y_train_res, X_val, y_val, X_test, y_test,\n",
        "                                         n_epochs=EPOCHS_TAB, batch_size=BATCH_TAB, device=None):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = TabTransformerTorch(input_dim=X_train_res.shape[1]).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_ds = TabDatasetTorch(X_train_res, y_train_res)\n",
        "    val_ds = TabDatasetTorch(X_val, y_val)\n",
        "    test_ds = TabDatasetTorch(X_test, y_test)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    best_val_loss = np.inf\n",
        "    best_state = None\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits, _, _ = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "        # validation loss\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits, _, _ = model(xb)\n",
        "                loss = loss_fn(logits, yb)\n",
        "                val_losses.append(loss.item())\n",
        "        mean_val_loss = np.mean(val_losses) if len(val_losses)>0 else np.nan\n",
        "        print(f\"TabTransformer epoch {epoch+1}/{n_epochs} train_loss={total_loss/len(train_loader):.4f} val_loss={mean_val_loss:.4f}\")\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best_val_loss = mean_val_loss\n",
        "            best_state = model.state_dict().copy()\n",
        "\n",
        "    # load best\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # evaluate on test\n",
        "    model.eval()\n",
        "    all_probs, all_labels, attn_list = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            logits, attn_weights, _ = model(xb)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(yb.numpy().flatten())\n",
        "\n",
        "            # robust attention aggregation\n",
        "            attn_np = attn_weights.cpu().numpy()\n",
        "            if attn_np.ndim == 4:      # (batch, heads, f, f)\n",
        "                attn_np2 = attn_np.mean(axis=1)        # -> (batch, f, f)\n",
        "                attn_feature_importance = attn_np2.mean(axis=(0,2))  # -> (f,)\n",
        "            elif attn_np.ndim == 3:    # (batch, f, f)\n",
        "                attn_feature_importance = attn_np.mean(axis=(0,2))\n",
        "            elif attn_np.ndim == 2:    # (batch, f)\n",
        "                attn_feature_importance = attn_np.mean(axis=0)\n",
        "            else:\n",
        "                raise ValueError(\"Unexpected attn_weights shape: {}\".format(attn_np.shape))\n",
        "            attn_list.append(attn_feature_importance)\n",
        "\n",
        "    probs = np.concatenate(all_probs)\n",
        "    labels = np.concatenate(all_labels)\n",
        "    metrics = compute_metrics(labels, probs)\n",
        "    cm = confusion_matrix(labels, (probs >= 0.5).astype(int))\n",
        "    attn_mean = np.mean(np.vstack(attn_list), axis=0)\n",
        "    return metrics, cm, (labels, probs), attn_mean\n",
        "\n",
        "metrics_tab, cm_tab, (labels_tab, probs_tab), attn_mean = evaluate_tabtransformer_single_split(\n",
        "    X_train_res, y_train_res, X_val, y_val_raw, X_test, y_test_raw,\n",
        "    n_epochs=EPOCHS_TAB, batch_size=BATCH_TAB\n",
        ")\n",
        "results['TabTransformer'] = metrics_tab\n",
        "print(\"TabTransformer metrics:\", metrics_tab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPkKfSPDgyDu",
        "outputId": "8bca8afb-3b26-4412-872a-7a307919ef29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TabTransformer epoch 1/50 train_loss=0.4559 val_loss=0.6062\n",
            "TabTransformer epoch 2/50 train_loss=0.3221 val_loss=0.3518\n",
            "TabTransformer epoch 3/50 train_loss=0.2757 val_loss=0.3180\n",
            "TabTransformer epoch 4/50 train_loss=0.2668 val_loss=0.4317\n",
            "TabTransformer epoch 5/50 train_loss=0.2638 val_loss=0.3209\n",
            "TabTransformer epoch 6/50 train_loss=0.2615 val_loss=0.2847\n",
            "TabTransformer epoch 7/50 train_loss=0.2593 val_loss=0.2977\n",
            "TabTransformer epoch 8/50 train_loss=0.2562 val_loss=0.3342\n",
            "TabTransformer epoch 9/50 train_loss=0.2550 val_loss=0.3216\n",
            "TabTransformer epoch 10/50 train_loss=0.2656 val_loss=0.3478\n",
            "TabTransformer epoch 11/50 train_loss=0.2533 val_loss=0.3243\n",
            "TabTransformer epoch 12/50 train_loss=0.2662 val_loss=0.3142\n",
            "TabTransformer epoch 13/50 train_loss=0.2576 val_loss=0.3029\n",
            "TabTransformer epoch 14/50 train_loss=0.2630 val_loss=0.3445\n",
            "TabTransformer epoch 15/50 train_loss=0.2505 val_loss=0.2752\n",
            "TabTransformer epoch 16/50 train_loss=0.2527 val_loss=0.3209\n",
            "TabTransformer epoch 17/50 train_loss=0.2481 val_loss=0.2723\n",
            "TabTransformer epoch 18/50 train_loss=0.2568 val_loss=0.3107\n",
            "TabTransformer epoch 19/50 train_loss=0.2459 val_loss=0.3142\n",
            "TabTransformer epoch 20/50 train_loss=0.2888 val_loss=0.3102\n",
            "TabTransformer epoch 21/50 train_loss=0.2878 val_loss=0.3287\n",
            "TabTransformer epoch 22/50 train_loss=0.2980 val_loss=0.3389\n",
            "TabTransformer epoch 23/50 train_loss=0.2842 val_loss=0.3271\n",
            "TabTransformer epoch 24/50 train_loss=0.2812 val_loss=0.3015\n",
            "TabTransformer epoch 25/50 train_loss=0.2779 val_loss=0.3183\n",
            "TabTransformer epoch 26/50 train_loss=0.2850 val_loss=0.3881\n",
            "TabTransformer epoch 27/50 train_loss=0.3047 val_loss=0.3444\n",
            "TabTransformer epoch 28/50 train_loss=0.2868 val_loss=0.3369\n",
            "TabTransformer epoch 29/50 train_loss=0.2713 val_loss=0.3416\n",
            "TabTransformer epoch 30/50 train_loss=0.2620 val_loss=0.3003\n",
            "TabTransformer epoch 31/50 train_loss=0.2548 val_loss=0.3355\n",
            "TabTransformer epoch 32/50 train_loss=0.2506 val_loss=0.3098\n",
            "TabTransformer epoch 33/50 train_loss=0.2467 val_loss=0.3171\n",
            "TabTransformer epoch 34/50 train_loss=0.2435 val_loss=0.2830\n",
            "TabTransformer epoch 35/50 train_loss=0.2436 val_loss=0.2732\n",
            "TabTransformer epoch 36/50 train_loss=0.2399 val_loss=0.2941\n",
            "TabTransformer epoch 37/50 train_loss=0.2385 val_loss=0.2959\n",
            "TabTransformer epoch 38/50 train_loss=0.2400 val_loss=0.2957\n",
            "TabTransformer epoch 39/50 train_loss=0.2530 val_loss=0.3054\n",
            "TabTransformer epoch 40/50 train_loss=0.2583 val_loss=0.3171\n",
            "TabTransformer epoch 41/50 train_loss=0.2481 val_loss=0.2728\n",
            "TabTransformer epoch 42/50 train_loss=0.2335 val_loss=0.2816\n",
            "TabTransformer epoch 43/50 train_loss=0.2286 val_loss=0.3196\n",
            "TabTransformer epoch 44/50 train_loss=0.2283 val_loss=0.2836\n",
            "TabTransformer epoch 45/50 train_loss=0.2280 val_loss=0.2932\n",
            "TabTransformer epoch 46/50 train_loss=0.2275 val_loss=0.2873\n",
            "TabTransformer epoch 47/50 train_loss=0.2279 val_loss=0.2969\n",
            "TabTransformer epoch 48/50 train_loss=0.2366 val_loss=0.3035\n",
            "TabTransformer epoch 49/50 train_loss=0.2277 val_loss=0.2855\n",
            "TabTransformer epoch 50/50 train_loss=0.2246 val_loss=0.2785\n",
            "TabTransformer metrics: {'acc': 0.8826868495742668, 'prec': 0.358974358974359, 'rec': 0.3125, 'f1': 0.3341288782816229, 'auc': 0.8226182383906293, 'ap': 0.30722786819377224, 'brier': 0.0849527467556698}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Oik6qnpQgyAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 10: Explainability (SHAP for XGB, Attention for TabTransformer)\n",
        "# SHAP (may be slow; sample test)"
      ],
      "metadata": {
        "id": "VepEF53Vi2E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Section 10: Explainability (SHAP for XGB, Attention for TabTransformer)\n",
        "# SHAP (may be slow; sample test)\n",
        "try:\n",
        "    sample_n = min(2000, X_test.shape[0])\n",
        "    explainer = shap.TreeExplainer(xgb_model)\n",
        "    shap_values = explainer.shap_values(X_test[:sample_n])\n",
        "    # Save SHAP plot\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values, X_test[:sample_n], feature_names=FEAT_NAMES, show=False)\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"shap_summary_xgb.png\"), dpi=300); plt.close()\n",
        "except Exception as e:\n",
        "    print(\"SHAP failed or slow:\", e)\n",
        "\n",
        "# Tab attention\n",
        "df_att = pd.DataFrame({\"feature\": FEAT_NAMES, \"attn\": attn_mean})\n",
        "df_att = df_att.sort_values(\"attn\", ascending=False)\n",
        "df_att.to_csv(os.path.join(OUTDIR,\"tab_attention.csv\"), index=False)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x=\"attn\", y=\"feature\", data=df_att.head(15))\n",
        "plt.title(\"Top 15 Features by TabTransformer Attention\")\n",
        "plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"tab_attention_top15.png\"), dpi=300); plt.close()\n",
        "\n",
        "# SHAP vs Attention comparison (top 10)\n",
        "try:\n",
        "    shap_imp = np.abs(shap_values).mean(axis=0)\n",
        "    df_shap = pd.DataFrame({\"feature\": FEAT_NAMES, \"shap\": shap_imp}).sort_values(\"shap\", ascending=False)\n",
        "    df_compare = pd.DataFrame({\n",
        "        \"SHAP_top10_feature\": df_shap[\"feature\"].head(10).values,\n",
        "        \"SHAP_value\": df_shap[\"shap\"].head(10).values,\n",
        "        \"ATTN_top10_feature\": df_att[\"feature\"].head(10).values,\n",
        "        \"ATTN_value\": df_att[\"attn\"].head(10).values\n",
        "    })\n",
        "    df_compare.to_csv(os.path.join(OUTDIR,\"explain_compare_top10.csv\"), index=False)\n",
        "except Exception as e:\n",
        "    print(\"SHAP vs Attn compare failed:\", e)\n"
      ],
      "metadata": {
        "id": "pMKKOiKfiU7y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 11: Visualization & final table"
      ],
      "metadata": {
        "id": "s__Mn6hXiozm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 11: Visualization & final table\n",
        "probs_dict = {\n",
        "    \"LogReg\": probs_log,\n",
        "    \"RandomForest\": probs_rf,\n",
        "    \"XGBoost\": probs_xgb,\n",
        "    \"LightGBM\": probs_lgb,\n",
        "    \"ANN\": probs_ann,\n",
        "    \"DeepMLP\": probs_dmlp,\n",
        "    \"TabTransformer\": probs_tab\n",
        "}\n",
        "\n",
        "# ROC\n",
        "plt.figure(figsize=(7,6))\n",
        "for name,p in probs_dict.items():\n",
        "    fpr,tpr,_ = roc_curve(y_test_raw, p)\n",
        "    aucv = roc_auc_score(y_test_raw, p)\n",
        "    plt.plot(fpr,tpr,label=f\"{name} (AUC={aucv:.3f})\")\n",
        "plt.plot([0,1],[0,1],'k--'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC Curves (Test)\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR,\"roc_all.png\"), dpi=300); plt.close()\n",
        "\n",
        "# PR\n",
        "plt.figure(figsize=(7,6))\n",
        "for name,p in probs_dict.items():\n",
        "    prec,rec,_ = precision_recall_curve(y_test_raw, p)\n",
        "    ap = average_precision_score(y_test_raw, p)\n",
        "    plt.plot(rec,prec,label=f\"{name} (AP={ap:.3f})\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR Curves (Test)\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR,\"pr_all.png\"), dpi=300); plt.close()\n",
        "\n",
        "# Calibration (LogReg, XGB, Tab)\n",
        "plt.figure(figsize=(6,6))\n",
        "for name in [\"LogReg\",\"XGBoost\",\"TabTransformer\"]:\n",
        "    p = probs_dict[name]\n",
        "    frac_pos, mean_pred = calibration_curve(y_test_raw, p, n_bins=10)\n",
        "    plt.plot(mean_pred, frac_pos, marker='o', label=name)\n",
        "plt.plot([0,1],[0,1],'k--'); plt.xlabel(\"Mean predicted prob\"); plt.ylabel(\"Fraction positives\"); plt.title(\"Calibration\"); plt.legend(); plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTDIR,\"calibration.png\"), dpi=300); plt.close()\n",
        "\n",
        "# Confusion matrices saved\n",
        "cms = {\"LogReg\":cm_log,\"RandomForest\":cm_rf,\"XGBoost\":cm_xgb,\"LightGBM\":cm_lgb,\"ANN\":cm_ann,\"DeepMLP\":cm_dmlp,\"TabTransformer\":cm_tab}\n",
        "for name,cm in cms.items():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pred 0\",\"Pred 1\"], yticklabels=[\"True 0\",\"True 1\"])\n",
        "    plt.title(f\"{name} Confusion Matrix\"); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,f\"cm_{name}.png\"), dpi=300); plt.close()\n",
        "\n",
        "# Final table\n",
        "df_final = pd.DataFrame(results).T\n",
        "df_final.to_csv(os.path.join(OUTDIR,\"final_results_table.csv\"))\n",
        "print(\"Final results table:\\n\", df_final)\n",
        "\n",
        "# Bootstrap AUC CI for Tab and XGB (test set)\n",
        "def bootstrap_auc_ci(y_true, probs, n_boot=2000):\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    aucs = []\n",
        "    n = len(y_true)\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, n)\n",
        "        if len(np.unique(y_true[idx]))<2:\n",
        "            continue\n",
        "        aucs.append(roc_auc_score(y_true[idx], probs[idx]))\n",
        "    return np.mean(aucs), (np.percentile(aucs,2.5), np.percentile(aucs,97.5))\n",
        "\n",
        "tab_mean_auc, tab_ci = bootstrap_auc_ci(y_test_raw, probs_tab)\n",
        "xgb_mean_auc, xgb_ci = bootstrap_auc_ci(y_test_raw, probs_xgb)\n",
        "print(f\"TabTransformer AUC {tab_mean_auc:.4f}, 95% CI {tab_ci}\")\n",
        "print(f\"XGBoost AUC       {xgb_mean_auc:.4f}, 95% CI {xgb_ci}\")\n",
        "\n",
        "# Save run metadata\n",
        "meta = {\"seed\":SEED, \"n_samples\":X_train.shape[0]+X_val.shape[0]+X_test.shape[0], \"n_features\": X_train.shape[1],\n",
        "        \"train_shape\": X_train_res.shape, \"val_shape\": X_val.shape, \"test_shape\": X_test.shape,\n",
        "        \"epochs_tab\": EPOCHS_TAB, \"epochs_deepmlp\": EPOCHS_DEEPMLP}\n",
        "with open(os.path.join(OUTDIR,\"run_meta.json\"), \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"All outputs saved to\", OUTDIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruWtpji1iU-l",
        "outputId": "8cb3f5be-c17d-4118-a0ea-2edcdca9734c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results table:\n",
            "                      acc      prec       rec        f1       auc        ap  \\\n",
            "LogReg          0.754809  0.247406  0.785156  0.376254  0.844712  0.361958   \n",
            "RandomForest    0.896300  0.397971  0.196987  0.263531  0.823814  0.305898   \n",
            "XGBoost         0.905971  0.502959  0.142299  0.221836  0.844094  0.357719   \n",
            "LightGBM        0.905051  0.488519  0.172154  0.254590  0.844993  0.360254   \n",
            "ANN             0.761773  0.251338  0.772879  0.379322  0.843956  0.358666   \n",
            "DeepMLP         0.861479  0.333531  0.471540  0.390706  0.828693  0.331464   \n",
            "TabTransformer  0.882687  0.358974  0.312500  0.334129  0.822618  0.307228   \n",
            "\n",
            "                   brier  \n",
            "LogReg          0.162647  \n",
            "RandomForest    0.077457  \n",
            "XGBoost         0.071370  \n",
            "LightGBM        0.071725  \n",
            "ANN             0.156121  \n",
            "DeepMLP         0.096169  \n",
            "TabTransformer  0.084953  \n",
            "TabTransformer AUC 0.8226, 95% CI (np.float64(0.8159097965962926), np.float64(0.8290183304208256))\n",
            "XGBoost AUC       0.8441, 95% CI (np.float64(0.8380731657796737), np.float64(0.849891216114594))\n",
            "All outputs saved to outputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HADQn6SZiVBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QS-F-QHsiVD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCn33SZ5iVHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEO4vGFLgxw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiEliTus7U8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Final consolidated pipeline (single script)\n",
        "- Preprocessing (impute+scale), 70/15/15 split\n",
        "- SMOTE on training only\n",
        "- Models: LogReg, RF, XGB, LGBM, ANN, DeepMLP (focal loss), TabTransformer (PyTorch, optional focal loss)\n",
        "- Ensemble (soft voting)\n",
        "- Threshold tuning (val set maximize F1)\n",
        "- Save results, plots, explainability artifacts\n",
        "\"\"\"\n",
        "\n",
        "# =========================\n",
        "# Section 0 — Imports & Config\n",
        "# =========================\n",
        "import os, json, warnings, time\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, average_precision_score, brier_score_loss,\n",
        "                             confusion_matrix, roc_curve, precision_recall_curve)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "# PyTorch for TabTransformer\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Explainability\n",
        "import shap\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# I/O\n",
        "OUTDIR = \"outputs\"\n",
        "os.makedirs(OUTDIR, exist_ok=True)\n",
        "\n",
        "# runtime knobs (change for final run)\n",
        "EPOCHS_TAB = 50\n",
        "EPOCHS_DEEPMLP = 50\n",
        "BATCH_TAB = 512\n",
        "BATCH_DEEPMLP = 512\n",
        "\n",
        "# =========================\n",
        "# Section 1 — Load dataset & features\n",
        "# =========================\n",
        "DATA_PATH = \"/content/heart_disease_health_indicators_BRFSS2015.csv\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Raw shape:\", df.shape)\n",
        "\n",
        "binary_cols = [\"HighBP\",\"HighChol\",\"CholCheck\",\"Smoker\",\"Stroke\",\"Diabetes\",\n",
        "               \"PhysActivity\",\"Fruits\",\"Veggies\",\"HvyAlcoholConsump\",\n",
        "               \"AnyHealthcare\",\"NoDocbcCost\",\"DiffWalk\",\"Sex\"]\n",
        "\n",
        "numeric_cols = [\"BMI\",\"MentHlth\",\"PhysHlth\",\"Age\"]\n",
        "categorical_cols = [\"GenHlth\",\"Education\",\"Income\"]   # keep as integer labels\n",
        "\n",
        "ALL_FEATURES = binary_cols + numeric_cols + categorical_cols\n",
        "TARGET = \"HeartDiseaseorAttack\"\n",
        "\n",
        "X_raw = df[ALL_FEATURES].copy()\n",
        "y_raw = df[TARGET].values.astype(int)\n",
        "print(\"Using features (count):\", len(ALL_FEATURES))\n",
        "\n",
        "# =========================\n",
        "# Section 2 — 70/15/15 Split (train/val/test)\n",
        "# =========================\n",
        "X_train_raw, X_temp_raw, y_train_raw, y_temp_raw = train_test_split(\n",
        "    X_raw, y_raw, test_size=0.30, stratify=y_raw, random_state=SEED\n",
        ")\n",
        "X_val_raw, X_test_raw, y_val_raw, y_test_raw = train_test_split(\n",
        "    X_temp_raw, y_temp_raw, test_size=0.50, stratify=y_temp_raw, random_state=SEED\n",
        ")\n",
        "print(\"Split sizes (train/val/test):\", X_train_raw.shape, X_val_raw.shape, X_test_raw.shape)\n",
        "\n",
        "# =========================\n",
        "# Section 3 — Preprocessing with Imputation (fit on train only)\n",
        "# =========================\n",
        "num_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())])\n",
        "cat_transformer = Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\"))])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    (\"num\", num_transformer, numeric_cols),\n",
        "    (\"cat\", cat_transformer, binary_cols + categorical_cols)\n",
        "], remainder=\"drop\")\n",
        "\n",
        "preprocessor.fit(X_train_raw)\n",
        "\n",
        "X_train = preprocessor.transform(X_train_raw)\n",
        "X_val   = preprocessor.transform(X_val_raw)\n",
        "X_test  = preprocessor.transform(X_test_raw)\n",
        "\n",
        "# ensure dense arrays\n",
        "if hasattr(X_train, \"toarray\"):\n",
        "    X_train = X_train.toarray(); X_val = X_val.toarray(); X_test = X_test.toarray()\n",
        "\n",
        "# save feature order (numeric scaled first, then passthrough)\n",
        "FEAT_NAMES = numeric_cols + (binary_cols + categorical_cols)\n",
        "pd.Series(FEAT_NAMES).to_csv(os.path.join(OUTDIR,\"features_used.csv\"), index=False)\n",
        "\n",
        "print(\"Processed shapes:\", X_train.shape, X_val.shape, X_test.shape)\n",
        "print(\"NaNs after preprocess:\", np.isnan(X_train).sum(), np.isnan(X_val).sum(), np.isnan(X_test).sum())\n",
        "\n",
        "# =========================\n",
        "# Section 4 — SMOTE on training set only\n",
        "# =========================\n",
        "sm = SMOTE(random_state=SEED)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train_raw.astype(int))\n",
        "print(\"After SMOTE:\", X_train_res.shape, np.bincount(y_train_res.astype(int)))\n",
        "\n",
        "# =========================\n",
        "# Section 5 — helpers (metrics, threshold tuning)\n",
        "# =========================\n",
        "def compute_metrics(y_true, probs, thresh=0.5):\n",
        "    preds = (probs >= thresh).astype(int)\n",
        "    return {\n",
        "        \"acc\": float(accuracy_score(y_true, preds)),\n",
        "        \"prec\": float(precision_score(y_true, preds, zero_division=0)),\n",
        "        \"rec\": float(recall_score(y_true, preds, zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, preds, zero_division=0)),\n",
        "        \"auc\": float(roc_auc_score(y_true, probs)),\n",
        "        \"ap\": float(average_precision_score(y_true, probs)),\n",
        "        \"brier\": float(brier_score_loss(y_true, probs))\n",
        "    }\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "def find_best_threshold(y_true, y_probs):\n",
        "    best_t, best_f1 = 0.5, -1\n",
        "    for t in np.linspace(0.05, 0.95, 91):\n",
        "        f1 = f1_score(y_true, (y_probs>=t).astype(int), zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    return best_t, best_f1\n",
        "\n",
        "# containers\n",
        "results_baseline = {}\n",
        "results_optimized = {}\n",
        "\n",
        "def eval_and_store(name, probs_val, probs_test, y_val, y_test):\n",
        "    # baseline (0.5)\n",
        "    metrics_base = compute_metrics(y_test, probs_test, 0.5)\n",
        "    results_baseline[name] = metrics_base\n",
        "    # optimized threshold using val set\n",
        "    best_t, best_f1 = find_best_threshold(y_val, probs_val)\n",
        "    metrics_opt = compute_metrics(y_test, probs_test, best_t)\n",
        "    results_optimized[name] = metrics_opt\n",
        "    print(f\"{name} | best_t={best_t:.3f} (val F1={best_f1:.3f}) | base_f1={metrics_base['f1']:.3f} opt_f1={metrics_opt['f1']:.3f}\")\n",
        "    return best_t\n",
        "\n",
        "# =========================\n",
        "# Section 6 — Classical ML: LogReg, RF, XGB, LGBM\n",
        "# =========================\n",
        "print(\"\\nTraining classical models...\")\n",
        "\n",
        "# Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=SEED, class_weight=\"balanced\")\n",
        "logreg.fit(X_train_res, y_train_res)\n",
        "probs_log_test = logreg.predict_proba(X_test)[:,1]\n",
        "probs_log_val  = logreg.predict_proba(X_val)[:,1]\n",
        "eval_and_store(\"LogReg\", probs_log_val, probs_log_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=300, random_state=SEED, class_weight=\"balanced\", n_jobs=-1)\n",
        "rf.fit(X_train_res, y_train_res)\n",
        "probs_rf_test = rf.predict_proba(X_test)[:,1]\n",
        "probs_rf_val  = rf.predict_proba(X_val)[:,1]\n",
        "eval_and_store(\"RandomForest\", probs_rf_val, probs_rf_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=SEED, n_jobs=-1)\n",
        "xgb_model.fit(X_train_res, y_train_res)\n",
        "probs_xgb_test = xgb_model.predict_proba(X_test)[:,1]\n",
        "probs_xgb_val  = xgb_model.predict_proba(X_val)[:,1]\n",
        "eval_and_store(\"XGBoost\", probs_xgb_val, probs_xgb_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(random_state=SEED)\n",
        "lgb_model.fit(X_train_res, y_train_res)\n",
        "probs_lgb_test = lgb_model.predict_proba(X_test)[:,1]\n",
        "probs_lgb_val  = lgb_model.predict_proba(X_val)[:,1]\n",
        "eval_and_store(\"LightGBM\", probs_lgb_val, probs_lgb_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# =========================\n",
        "# Section 7 — Neural baselines (Keras): ANN and DeepMLP with focal loss\n",
        "# =========================\n",
        "print(\"\\nTraining ANN + DeepMLP (Keras)...\")\n",
        "\n",
        "def focal_loss_keras(gamma=2., alpha=0.25):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = K.cast(y_true, \"float32\")\n",
        "        bce = K.binary_crossentropy(y_true, y_pred)\n",
        "        p_t = y_true * y_pred + (1 - y_true) * (1 - y_pred)\n",
        "        alpha_factor = y_true * alpha + (1 - y_true) * (1 - alpha)\n",
        "        modulating = K.pow((1 - p_t), gamma)\n",
        "        return K.mean(alpha_factor * modulating * bce)\n",
        "    return loss\n",
        "\n",
        "input_dim = X_train_res.shape[1]\n",
        "\n",
        "# Simple ANN (baseline)\n",
        "ann = Sequential([Dense(32, activation='relu', input_shape=(input_dim,)), Dense(1, activation='sigmoid')])\n",
        "ann.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "ann.fit(X_train_res, y_train_res, validation_data=(X_val, y_val_raw), epochs=5, batch_size=512, verbose=1)\n",
        "probs_ann_test = ann.predict(X_test).flatten()\n",
        "probs_ann_val  = ann.predict(X_val).flatten()\n",
        "eval_and_store(\"ANN\", probs_ann_val, probs_ann_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# Deep MLP with focal loss\n",
        "deep_mlp = Sequential([\n",
        "    Dense(256, activation='relu', input_shape=(input_dim,)), BatchNormalization(), Dropout(0.3),\n",
        "    Dense(128, activation='relu'), BatchNormalization(), Dropout(0.3),\n",
        "    Dense(64, activation='relu'), BatchNormalization(), Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "deep_mlp.compile(optimizer='adam', loss=focal_loss_keras(gamma=2., alpha=0.25))\n",
        "deep_mlp.fit(X_train_res, y_train_res, validation_data=(X_val, y_val_raw), epochs=EPOCHS_DEEPMLP, batch_size=BATCH_DEEPMLP, verbose=1)\n",
        "probs_dmlp_test = deep_mlp.predict(X_test).flatten()\n",
        "probs_dmlp_val  = deep_mlp.predict(X_val).flatten()\n",
        "eval_and_store(\"DeepMLP\", probs_dmlp_val, probs_dmlp_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# =========================\n",
        "# Section 8 — Ensemble (soft voting of XGB+RF+LGB)\n",
        "# =========================\n",
        "print(\"\\nTraining Voting Ensemble (XGB+RF+LGB)...\")\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        (\"xgb\", xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=SEED)),\n",
        "        (\"rf\", RandomForestClassifier(n_estimators=300, random_state=SEED, class_weight=\"balanced\")),\n",
        "        (\"lgb\", lgb.LGBMClassifier(random_state=SEED))\n",
        "    ],\n",
        "    voting=\"soft\", n_jobs=-1\n",
        ")\n",
        "ensemble.fit(X_train_res, y_train_res)\n",
        "probs_ens_test = ensemble.predict_proba(X_test)[:,1]\n",
        "probs_ens_val  = ensemble.predict_proba(X_val)[:,1]\n",
        "eval_and_store(\"Ensemble\", probs_ens_val, probs_ens_test, y_val_raw, y_test_raw)\n",
        "\n",
        "# =========================\n",
        "# Section 9 — TabTransformer (PyTorch) — single split, focal loss optional\n",
        "# =========================\n",
        "print(\"\\nTraining TabTransformer (PyTorch)...\")\n",
        "\n",
        "class TabDatasetTorch(Dataset):\n",
        "    def __init__(self, X_np, y_np):\n",
        "        self.X = torch.tensor(X_np, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y_np, dtype=torch.float32).unsqueeze(1)\n",
        "    def __len__(self): return len(self.y)\n",
        "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
        "\n",
        "class TabTransformerTorch(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim=16, n_heads=2, mlp_hidden=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(1, emb_dim)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=emb_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.ff = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.ReLU())\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_dim*emb_dim, mlp_hidden), nn.ReLU(), nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        b,f = x.shape\n",
        "        x = x.unsqueeze(-1)            # (b,f,1)\n",
        "        emb = self.proj(x)             # (b,f,emb_dim)\n",
        "        attn_out, attn_weights = self.mha(emb, emb, emb, need_weights=True, average_attn_weights=False)\n",
        "        out = self.ff(emb + attn_out)\n",
        "        flat = out.reshape(b, -1)\n",
        "        logits = self.classifier(flat)\n",
        "        return logits, attn_weights, out\n",
        "\n",
        "# PyTorch focal loss\n",
        "class FocalLossTorch(nn.Module):\n",
        "    def __init__(self, alpha=0.25, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "    def forward(self, logits, targets):\n",
        "        bce = nn.BCEWithLogitsLoss(reduction='none')(logits, targets)\n",
        "        probs = torch.sigmoid(logits)\n",
        "        pt = torch.where(targets == 1, probs, 1 - probs)\n",
        "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
        "        return focal.mean()\n",
        "\n",
        "def train_tabtransformer(X_train_res, y_train_res, X_val, y_val, X_test, y_test,\n",
        "                         n_epochs=EPOCHS_TAB, batch_size=BATCH_TAB, use_focal=False, device=None):\n",
        "    if device is None:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = TabTransformerTorch(input_dim=X_train_res.shape[1]).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = FocalLossTorch(alpha=0.25, gamma=2.0) if use_focal else nn.BCEWithLogitsLoss()\n",
        "\n",
        "    train_ds = TabDatasetTorch(X_train_res, y_train_res)\n",
        "    val_ds = TabDatasetTorch(X_val, y_val)\n",
        "    test_ds = TabDatasetTorch(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
        "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    best_val_loss = np.inf; best_state = None\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train(); total_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits, _, _ = model(xb)\n",
        "            loss = loss_fn(logits, yb)\n",
        "            loss.backward(); opt.step()\n",
        "            total_loss += loss.item()\n",
        "        # val\n",
        "        model.eval(); val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                logits, _, _ = model(xb)\n",
        "                val_losses.append(loss_fn(logits,yb).item())\n",
        "        mean_val_loss = np.mean(val_losses) if len(val_losses)>0 else np.nan\n",
        "        print(f\"Tab epoch {epoch+1}/{n_epochs} train_loss={total_loss/len(train_loader):.4f} val_loss={mean_val_loss:.4f}\")\n",
        "        if mean_val_loss < best_val_loss:\n",
        "            best_val_loss = mean_val_loss\n",
        "            best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # test inference\n",
        "    model.eval()\n",
        "    all_probs, all_labels, attn_maps = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            logits, attn_weights, _ = model(xb)\n",
        "            probs = torch.sigmoid(logits).cpu().numpy().flatten()\n",
        "            all_probs.append(probs)\n",
        "            all_labels.append(yb.numpy().flatten())\n",
        "\n",
        "            # robust attn aggregation\n",
        "            attn_np = attn_weights.cpu().numpy()\n",
        "            if attn_np.ndim == 4:\n",
        "                attn_np2 = attn_np.mean(axis=1)     # (batch, f, f)\n",
        "                attn_feat = attn_np2.mean(axis=(0,2))\n",
        "            elif attn_np.ndim == 3:\n",
        "                attn_feat = attn_np.mean(axis=(0,2))\n",
        "            elif attn_np.ndim == 2:\n",
        "                attn_feat = attn_np.mean(axis=0)\n",
        "            else:\n",
        "                raise ValueError(\"Unexpected attn shape: {}\".format(attn_np.shape))\n",
        "            attn_maps.append(attn_feat)\n",
        "\n",
        "    probs_test = np.concatenate(all_probs)\n",
        "    labels_test = np.concatenate(all_labels)\n",
        "    metrics = compute_metrics(labels_test, probs_test)\n",
        "    cm = confusion_matrix(labels_test, (probs_test>=0.5).astype(int))\n",
        "    attn_mean = np.mean(np.vstack(attn_maps), axis=0) if len(attn_maps)>0 else np.zeros(X_train_res.shape[1])\n",
        "    return metrics, cm, (labels_test, probs_test), attn_mean, model\n",
        "\n",
        "metrics_tab, cm_tab, (labels_tab, probs_tab), attn_mean, tab_model = train_tabtransformer(\n",
        "    X_train_res, y_train_res, X_val, y_val_raw, X_test, y_test_raw,\n",
        "    n_epochs=EPOCHS_TAB, batch_size=BATCH_TAB, use_focal=True\n",
        ")\n",
        "eval_and_store(\"TabTransformer\", probs_tab, probs_tab, labels_tab, labels_tab)  # use val/test= same for tab (we used val during training)\n",
        "\n",
        "# save attention\n",
        "df_att = pd.DataFrame({\"feature\": FEAT_NAMES, \"attn\": attn_mean})\n",
        "df_att.sort_values(\"attn\", ascending=False).to_csv(os.path.join(OUTDIR,\"tab_attention.csv\"), index=False)\n",
        "plt.figure(figsize=(8,6)); sns.barplot(x=\"attn\", y=\"feature\", data=df_att.head(15)); plt.title(\"Top 15 Tab Attention\"); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"tab_attention_top15.png\")); plt.close()\n",
        "\n",
        "# =========================\n",
        "# Section 10 — SHAP for XGB (optional, may be slow)\n",
        "# =========================\n",
        "try:\n",
        "    sample_n = min(2000, X_test.shape[0])\n",
        "    explainer = shap.TreeExplainer(xgb_model)\n",
        "    shap_values = explainer.shap_values(X_test[:sample_n])\n",
        "    plt.figure()\n",
        "    shap.summary_plot(shap_values, X_test[:sample_n], feature_names=FEAT_NAMES, show=False)\n",
        "    plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"shap_summary_xgb.png\"), dpi=300); plt.close()\n",
        "except Exception as e:\n",
        "    print(\"SHAP skipped or failed:\", e)\n",
        "\n",
        "# =========================\n",
        "# Section 11 — Plots & Final Tables (Baseline & Optimized)\n",
        "# =========================\n",
        "probs_dict_test = {\n",
        "    \"LogReg\": probs_log_test,\n",
        "    \"RandomForest\": probs_rf_test,\n",
        "    \"XGBoost\": probs_xgb_test,\n",
        "    \"LightGBM\": probs_lgb_test,\n",
        "    \"ANN\": probs_ann_test,\n",
        "    \"DeepMLP\": probs_dmlp_test,\n",
        "    \"Ensemble\": probs_ens_test,\n",
        "    \"TabTransformer\": probs_tab\n",
        "}\n",
        "\n",
        "# ROC plot\n",
        "plt.figure(figsize=(8,6))\n",
        "for name, p in probs_dict_test.items():\n",
        "    fpr, tpr, _ = roc_curve(y_test_raw, p)\n",
        "    aucv = roc_auc_score(y_test_raw, p)\n",
        "    plt.plot(fpr, tpr, label=f\"{name} AUC={aucv:.3f}\")\n",
        "plt.plot([0,1],[0,1],'k--'); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (Test)\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"roc_all.png\")); plt.close()\n",
        "\n",
        "# PR plot\n",
        "plt.figure(figsize=(8,6))\n",
        "for name, p in probs_dict_test.items():\n",
        "    prec, rec, _ = precision_recall_curve(y_test_raw, p)\n",
        "    ap = average_precision_score(y_test_raw, p)\n",
        "    plt.plot(rec, prec, label=f\"{name} AP={ap:.3f}\")\n",
        "plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR (Test)\"); plt.legend(bbox_to_anchor=(1.05,1)); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"pr_all.png\")); plt.close()\n",
        "\n",
        "# Calibration (LogReg, XGB, Tab)\n",
        "plt.figure(figsize=(6,6))\n",
        "for name in [\"LogReg\",\"XGBoost\",\"TabTransformer\"]:\n",
        "    p = probs_dict_test[name]\n",
        "    frac_pos, mean_pred = calibration_curve(y_test_raw, p, n_bins=10)\n",
        "    plt.plot(mean_pred, frac_pos, marker='o', label=name)\n",
        "plt.plot([0,1],[0,1],'k--'); plt.xlabel(\"Mean pred prob\"); plt.ylabel(\"Fraction positives\"); plt.title(\"Calibration\"); plt.legend(); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"calibration.png\")); plt.close()\n",
        "\n",
        "# Confusion matrices\n",
        "cms = {\n",
        "    \"LogReg\": confusion_matrix(y_test_raw, (probs_log_test>=0.5).astype(int)),\n",
        "    \"RandomForest\": confusion_matrix(y_test_raw, (probs_rf_test>=0.5).astype(int)),\n",
        "    \"XGBoost\": confusion_matrix(y_test_raw, (probs_xgb_test>=0.5).astype(int)),\n",
        "    \"LightGBM\": confusion_matrix(y_test_raw, (probs_lgb_test>=0.5).astype(int)),\n",
        "    \"ANN\": confusion_matrix(y_test_raw, (probs_ann_test>=0.5).astype(int)),\n",
        "    \"DeepMLP\": confusion_matrix(y_test_raw, (probs_dmlp_test>=0.5).astype(int)),\n",
        "    \"Ensemble\": confusion_matrix(y_test_raw, (probs_ens_test>=0.5).astype(int)),\n",
        "    \"TabTransformer\": cm_tab\n",
        "}\n",
        "for name, cm in cms.items():\n",
        "    plt.figure(figsize=(4,3))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pred0\",\"Pred1\"], yticklabels=[\"True0\",\"True1\"])\n",
        "    plt.title(f\"{name} Confusion Matrix\"); plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,f\"cm_{name}.png\")); plt.close()\n",
        "\n",
        "# Final CSVs for baseline & optimized results\n",
        "df_base = pd.DataFrame(results_baseline).T\n",
        "df_opt  = pd.DataFrame(results_optimized).T\n",
        "df_base.to_csv(os.path.join(OUTDIR,\"results_baseline.csv\"))\n",
        "df_opt.to_csv(os.path.join(OUTDIR,\"results_optimized.csv\"))\n",
        "df_compare = df_base.add_suffix(\"_base\").merge(df_opt.add_suffix(\"_opt\"), left_index=True, right_index=True)\n",
        "df_compare.to_csv(os.path.join(OUTDIR,\"results_improved.csv\"))\n",
        "\n",
        "print(\"\\nBaseline (threshold=0.5):\\n\", df_base)\n",
        "print(\"\\nOptimized (val-tuned threshold):\\n\", df_opt)\n",
        "print(\"\\nSide-by-side saved to outputs/results_improved.csv\")\n",
        "\n",
        "# =========================\n",
        "# Section 12 — simple bootstrap for AUC CI (Tab vs XGB)\n",
        "# =========================\n",
        "def bootstrap_auc_ci(y, probs, n_boot=1000):\n",
        "    rng = np.random.RandomState(SEED)\n",
        "    aucs = []\n",
        "    n = len(y)\n",
        "    for i in range(n_boot):\n",
        "        idx = rng.randint(0, n, n)\n",
        "        if len(np.unique(y[idx])) < 2: continue\n",
        "        aucs.append(roc_auc_score(y[idx], probs[idx]))\n",
        "    return np.mean(aucs), (np.percentile(aucs, 2.5), np.percentile(aucs,97.5))\n",
        "\n",
        "tab_mu, tab_ci = bootstrap_auc_ci(y_test_raw, probs_tab)\n",
        "xgb_mu, xgb_ci = bootstrap_auc_ci(y_test_raw, probs_xgb_test)\n",
        "print(f\"TabTransformer AUC {tab_mu:.4f}, 95% CI {tab_ci}\")\n",
        "print(f\"XGBoost AUC       {xgb_mu:.4f}, 95% CI {xgb_ci}\")\n",
        "\n",
        "# Save metadata\n",
        "meta = {\n",
        "    \"seed\": SEED, \"n_samples\": int(X_train.shape[0] + X_val.shape[0] + X_test.shape[0]),\n",
        "    \"n_features\": int(X_train.shape[1]), \"train_shape\": X_train_res.shape, \"val_shape\": X_val.shape, \"test_shape\": X_test.shape,\n",
        "    \"epochs_tab\": EPOCHS_TAB, \"epochs_deepmlp\": EPOCHS_DEEPMLP\n",
        "}\n",
        "with open(os.path.join(OUTDIR,\"run_meta.json\"), \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"All outputs saved to\", OUTDIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0lptgEh7U_6",
        "outputId": "aa0008e2-6322-472b-dcf3-2421820e3f85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shape: (253680, 22)\n",
            "Using features (count): 21\n",
            "Split sizes (train/val/test): (177576, 21) (38052, 21) (38052, 21)\n",
            "Processed shapes: (177576, 21) (38052, 21) (38052, 21)\n",
            "NaNs after preprocess: 0 0 0\n",
            "After SMOTE: (321702, 21) [160851 160851]\n",
            "\n",
            "Training classical models...\n",
            "LogReg | best_t=0.690 (val F1=0.413) | base_f1=0.376 opt_f1=0.417\n",
            "RandomForest | best_t=0.220 (val F1=0.378) | base_f1=0.267 opt_f1=0.390\n",
            "XGBoost | best_t=0.210 (val F1=0.409) | base_f1=0.218 opt_f1=0.407\n",
            "[LightGBM] [Info] Number of positive: 160851, number of negative: 160851\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.045731 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 5355\n",
            "[LightGBM] [Info] Number of data points in the train set: 321702, number of used features: 21\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
            "LightGBM | best_t=0.240 (val F1=0.414) | base_f1=0.253 opt_f1=0.415\n",
            "\n",
            "Training ANN + DeepMLP (Keras)...\n",
            "Epoch 1/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 0.5084 - val_loss: 0.4874\n",
            "Epoch 2/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4658 - val_loss: 0.4773\n",
            "Epoch 3/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4598 - val_loss: 0.4704\n",
            "Epoch 4/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.4554 - val_loss: 0.4665\n",
            "Epoch 5/5\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.4517 - val_loss: 0.4647\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "ANN | best_t=0.680 (val F1=0.413) | base_f1=0.376 opt_f1=0.412\n",
            "Epoch 1/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 10ms/step - loss: 0.0903 - val_loss: 0.0463\n",
            "Epoch 2/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0523 - val_loss: 0.0434\n",
            "Epoch 3/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0511 - val_loss: 0.0428\n",
            "Epoch 4/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0504 - val_loss: 0.0446\n",
            "Epoch 5/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0497 - val_loss: 0.0461\n",
            "Epoch 6/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0490 - val_loss: 0.0424\n",
            "Epoch 7/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0481 - val_loss: 0.0422\n",
            "Epoch 8/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0471 - val_loss: 0.0380\n",
            "Epoch 9/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0463 - val_loss: 0.0412\n",
            "Epoch 10/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0453 - val_loss: 0.0374\n",
            "Epoch 11/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0443 - val_loss: 0.0365\n",
            "Epoch 12/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0433 - val_loss: 0.0345\n",
            "Epoch 13/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0426 - val_loss: 0.0350\n",
            "Epoch 14/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0415 - val_loss: 0.0362\n",
            "Epoch 15/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0408 - val_loss: 0.0339\n",
            "Epoch 16/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0401 - val_loss: 0.0314\n",
            "Epoch 17/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0396 - val_loss: 0.0291\n",
            "Epoch 18/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0312\n",
            "Epoch 19/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0340\n",
            "Epoch 20/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0319\n",
            "Epoch 21/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0296\n",
            "Epoch 22/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0332\n",
            "Epoch 23/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0344\n",
            "Epoch 24/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0373 - val_loss: 0.0286\n",
            "Epoch 25/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0369 - val_loss: 0.0307\n",
            "Epoch 26/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0368 - val_loss: 0.0313\n",
            "Epoch 27/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0363 - val_loss: 0.0283\n",
            "Epoch 28/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0313\n",
            "Epoch 29/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0290\n",
            "Epoch 30/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0292\n",
            "Epoch 31/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0290\n",
            "Epoch 32/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0350 - val_loss: 0.0300\n",
            "Epoch 33/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0290\n",
            "Epoch 34/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0345 - val_loss: 0.0289\n",
            "Epoch 35/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0345 - val_loss: 0.0327\n",
            "Epoch 36/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0343 - val_loss: 0.0290\n",
            "Epoch 37/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0342 - val_loss: 0.0299\n",
            "Epoch 38/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0340 - val_loss: 0.0298\n",
            "Epoch 39/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0335 - val_loss: 0.0287\n",
            "Epoch 40/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0336 - val_loss: 0.0304\n",
            "Epoch 41/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0336 - val_loss: 0.0293\n",
            "Epoch 42/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0334 - val_loss: 0.0316\n",
            "Epoch 43/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0335 - val_loss: 0.0314\n",
            "Epoch 44/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0330 - val_loss: 0.0289\n",
            "Epoch 45/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0329 - val_loss: 0.0295\n",
            "Epoch 46/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 0.0328 - val_loss: 0.0319\n",
            "Epoch 47/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0328 - val_loss: 0.0303\n",
            "Epoch 48/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0329 - val_loss: 0.0302\n",
            "Epoch 49/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0331 - val_loss: 0.0283\n",
            "Epoch 50/50\n",
            "\u001b[1m629/629\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0328 - val_loss: 0.0301\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\u001b[1m1190/1190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "DeepMLP | best_t=0.390 (val F1=0.389) | base_f1=0.245 opt_f1=0.391\n",
            "\n",
            "Training Voting Ensemble (XGB+RF+LGB)...\n",
            "Ensemble | best_t=0.230 (val F1=0.408) | base_f1=0.232 opt_f1=0.418\n",
            "\n",
            "Training TabTransformer (PyTorch)...\n",
            "Tab epoch 1/50 train_loss=0.0292 val_loss=0.0273\n",
            "Tab epoch 2/50 train_loss=0.0238 val_loss=0.0209\n",
            "Tab epoch 3/50 train_loss=0.0212 val_loss=0.0222\n",
            "Tab epoch 4/50 train_loss=0.0201 val_loss=0.0219\n",
            "Tab epoch 5/50 train_loss=0.0197 val_loss=0.0240\n",
            "Tab epoch 6/50 train_loss=0.0192 val_loss=0.0205\n",
            "Tab epoch 7/50 train_loss=0.0192 val_loss=0.0207\n",
            "Tab epoch 8/50 train_loss=0.0189 val_loss=0.0214\n",
            "Tab epoch 9/50 train_loss=0.0186 val_loss=0.0207\n",
            "Tab epoch 10/50 train_loss=0.0186 val_loss=0.0303\n",
            "Tab epoch 11/50 train_loss=0.0186 val_loss=0.0213\n",
            "Tab epoch 12/50 train_loss=0.0184 val_loss=0.0199\n",
            "Tab epoch 13/50 train_loss=0.0183 val_loss=0.0215\n",
            "Tab epoch 14/50 train_loss=0.0184 val_loss=0.0201\n",
            "Tab epoch 15/50 train_loss=0.0184 val_loss=0.0173\n",
            "Tab epoch 16/50 train_loss=0.0181 val_loss=0.0230\n",
            "Tab epoch 17/50 train_loss=0.0181 val_loss=0.0180\n",
            "Tab epoch 18/50 train_loss=0.0182 val_loss=0.0321\n",
            "Tab epoch 19/50 train_loss=0.0180 val_loss=0.0183\n",
            "Tab epoch 20/50 train_loss=0.0180 val_loss=0.0224\n",
            "Tab epoch 21/50 train_loss=0.0181 val_loss=0.0258\n",
            "Tab epoch 22/50 train_loss=0.0178 val_loss=0.0172\n",
            "Tab epoch 23/50 train_loss=0.0180 val_loss=0.0192\n",
            "Tab epoch 24/50 train_loss=0.0179 val_loss=0.0176\n",
            "Tab epoch 25/50 train_loss=0.0181 val_loss=0.0206\n",
            "Tab epoch 26/50 train_loss=0.0180 val_loss=0.0264\n",
            "Tab epoch 27/50 train_loss=0.0177 val_loss=0.0187\n",
            "Tab epoch 28/50 train_loss=0.0177 val_loss=0.0181\n",
            "Tab epoch 29/50 train_loss=0.0179 val_loss=0.0237\n",
            "Tab epoch 30/50 train_loss=0.0178 val_loss=0.0224\n",
            "Tab epoch 31/50 train_loss=0.0179 val_loss=0.0191\n",
            "Tab epoch 32/50 train_loss=0.0177 val_loss=0.0182\n",
            "Tab epoch 33/50 train_loss=0.0179 val_loss=0.0217\n",
            "Tab epoch 34/50 train_loss=0.0177 val_loss=0.0201\n",
            "Tab epoch 35/50 train_loss=0.0176 val_loss=0.0189\n",
            "Tab epoch 36/50 train_loss=0.0177 val_loss=0.0244\n",
            "Tab epoch 37/50 train_loss=0.0178 val_loss=0.0190\n",
            "Tab epoch 38/50 train_loss=0.0175 val_loss=0.0562\n",
            "Tab epoch 39/50 train_loss=0.0204 val_loss=0.0219\n",
            "Tab epoch 40/50 train_loss=0.0185 val_loss=0.0300\n",
            "Tab epoch 41/50 train_loss=0.0176 val_loss=0.0178\n",
            "Tab epoch 42/50 train_loss=0.0175 val_loss=0.0191\n",
            "Tab epoch 43/50 train_loss=0.0177 val_loss=0.0196\n",
            "Tab epoch 44/50 train_loss=0.0173 val_loss=0.0218\n",
            "Tab epoch 45/50 train_loss=0.0172 val_loss=0.0203\n",
            "Tab epoch 46/50 train_loss=0.0174 val_loss=0.0187\n",
            "Tab epoch 47/50 train_loss=0.0173 val_loss=0.0191\n",
            "Tab epoch 48/50 train_loss=0.0171 val_loss=0.0186\n",
            "Tab epoch 49/50 train_loss=0.0172 val_loss=0.0195\n",
            "Tab epoch 50/50 train_loss=0.0171 val_loss=0.0241\n",
            "TabTransformer | best_t=0.410 (val F1=0.395) | base_f1=0.317 opt_f1=0.395\n",
            "\n",
            "Baseline (threshold=0.5):\n",
            "                      acc      prec       rec        f1       auc        ap  \\\n",
            "LogReg          0.754862  0.247317  0.784319  0.376054  0.844694  0.361971   \n",
            "RandomForest    0.896615  0.401796  0.199777  0.266865  0.823190  0.305836   \n",
            "XGBoost         0.905997  0.503525  0.139509  0.218484  0.843623  0.358291   \n",
            "LightGBM        0.904893  0.486100  0.170759  0.252736  0.845118  0.360127   \n",
            "ANN             0.758436  0.248655  0.773996  0.376391  0.843994  0.359227   \n",
            "DeepMLP         0.902134  0.448071  0.168527  0.244931  0.829549  0.328368   \n",
            "Ensemble        0.904946  0.485359  0.152623  0.232222  0.845814  0.360890   \n",
            "TabTransformer  0.895590  0.412819  0.256975  0.316767  0.831412  0.326746   \n",
            "\n",
            "                   brier  \n",
            "LogReg          0.162620  \n",
            "RandomForest    0.077467  \n",
            "XGBoost         0.071343  \n",
            "LightGBM        0.071717  \n",
            "ANN             0.156273  \n",
            "DeepMLP         0.097682  \n",
            "Ensemble        0.071502  \n",
            "TabTransformer  0.098446  \n",
            "\n",
            "Optimized (val-tuned threshold):\n",
            "                      acc      prec       rec        f1       auc        ap  \\\n",
            "LogReg          0.848523  0.326929  0.574498  0.416717  0.844694  0.361971   \n",
            "RandomForest    0.816935  0.284366  0.622210  0.390338  0.823190  0.305836   \n",
            "XGBoost         0.838221  0.310827  0.589565  0.407051  0.843623  0.358291   \n",
            "LightGBM        0.851125  0.329398  0.560547  0.414954  0.845118  0.360127   \n",
            "ANN             0.846237  0.322140  0.572824  0.412373  0.843994  0.359227   \n",
            "DeepMLP         0.846578  0.312041  0.522042  0.390605  0.829549  0.328368   \n",
            "Ensemble        0.842768  0.320676  0.598493  0.417600  0.845814  0.360890   \n",
            "TabTransformer  0.853359  0.322924  0.507812  0.394794  0.831412  0.326746   \n",
            "\n",
            "                   brier  \n",
            "LogReg          0.162620  \n",
            "RandomForest    0.077467  \n",
            "XGBoost         0.071343  \n",
            "LightGBM        0.071717  \n",
            "ANN             0.156273  \n",
            "DeepMLP         0.097682  \n",
            "Ensemble        0.071502  \n",
            "TabTransformer  0.098446  \n",
            "\n",
            "Side-by-side saved to outputs/results_improved.csv\n",
            "TabTransformer AUC 0.8315, 95% CI (np.float64(0.825188296262691), np.float64(0.8373680619732355))\n",
            "XGBoost AUC       0.8437, 95% CI (np.float64(0.8378892639423823), np.float64(0.8494140344105114))\n",
            "All outputs saved to outputs\n"
          ]
        }
      ]
    }
  ]
}